{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO68fECfivBriKHpfW0Xp/I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/LORA_finetuning/blob/main/LORA_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://www.youtube.com/watch?v=D3pXSkGceY0&list=PLZAGXXsIV3P3gCenOWRd56ZpeksdYFUKV"
      ],
      "metadata": {
        "id": "0y7pNVIv4aMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-1: Test the model before fine-tuning for checking accuracy later"
      ],
      "metadata": {
        "id": "OOY_dZGazhRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-2: Get resources to fine-tune the model (usually text or pdfs)"
      ],
      "metadata": {
        "id": "wr0aqguN4e-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-3: Chunk the data (using Hybrid chunker for semantic chunking strategies) and use some really big model to generate basic question for the each chunk and structure them in question and answer format"
      ],
      "metadata": {
        "id": "DXbj4R9O4xp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "from docling.chunking import HybridChunker\n",
        "from colorama import Fore\n",
        "\n",
        "import json\n",
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "from litellm import completion\n",
        "\n",
        "prompt_template = \"\"\"You are an expert data curator assisting a machine learning engineer in creating a high-quality instruction tuning dataset. Your task is to transform\n",
        "    the provided data chunk into diverse question and answer (Q&A) pairs that will be used to fine-tune a language model.\n",
        "\n",
        "    For each of the {num_records} entries, generate one or two well-structured questions that reflect different aspects of the information in the chunk.\n",
        "    Ensure a mix of longer and shorter questions, with shorter ones typically containing 1-2 sentences and longer ones spanning up to 3-4 sentences. Each\n",
        "    Q&A pair should be concise yet informative, capturing key insights from the data.\n",
        "\n",
        "    Structure your output in JSON format, where each object contains 'question' and 'answer' fields. The JSON structure should look like this:\n",
        "\n",
        "        \"question\": \"Your question here...\",\n",
        "        \"answer\": \"Your answer here...\"\n",
        "\n",
        "    Focus on creating clear, relevant, and varied questions that encourage the model to learn from diverse perspectives. Avoid any sensitive or biased\n",
        "    content, ensuring answers are accurate and neutral.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        \"question\": \"What is the primary purpose of this dataset?\",\n",
        "        \"answer\": \"This dataset serves as training data for fine-tuning a language model.\"\n",
        "\n",
        "\n",
        "    By following these guidelines, you'll contribute to a robust and effective dataset that enhances the model's performance.\"\n",
        "\n",
        "    ---\n",
        "\n",
        "    **Explanation:**\n",
        "\n",
        "    - **Clarity and Specificity:** The revised prompt clearly defines the role of the assistant and the importance of the task, ensuring alignment with the\n",
        "    project goals.\n",
        "    - **Quality Standards:** It emphasizes the need for well-formulated Q&A pairs, specifying the structure and content of each question and answer.\n",
        "    - **Output Format:** An example JSON structure is provided to guide the format accurately.\n",
        "    - **Constraints and Biases:** A note on avoiding sensitive or biased content ensures ethical considerations are met.\n",
        "    - **Step-by-Step Guidance:** The prompt breaks down the task into manageable steps, making it easier for the assistant to follow.\n",
        "\n",
        "    This approach ensures that the generated data is both high-quality and meets the specific requirements of the machine learning project.\n",
        "\n",
        "    Data\n",
        "    {data}\n",
        "    \"\"\"\n",
        "\n",
        "class Record(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "\n",
        "class Response(BaseModel):\n",
        "    generated: List[Record]\n",
        "\n",
        "# call to convert data into question and answer\n",
        "def llm_call(data: str, num_records: int = 5) -> dict:\n",
        "    stream = completion(\n",
        "        model=\"ollama_chat/qwen2.5:14b\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt_template(data, num_records),\n",
        "            }\n",
        "        ],\n",
        "        stream=True,\n",
        "        options={\"num_predict\": 2000},\n",
        "        format=Response.model_json_schema(), # telling output format\n",
        "    )\n",
        "    data = \"\"\n",
        "    for x in stream:\n",
        "        delta = x['choices'][0][\"delta\"][\"content\"]\n",
        "        if delta is not None:\n",
        "            print(Fore.LIGHTBLUE_EX+ delta + Fore.RESET, end=\"\")\n",
        "            data += delta\n",
        "    return json.loads(data)\n",
        "\n",
        "# chunk input data from the pdf and convert into question and answer format for each chunk using a really big model\n",
        "if __name__ == \"__main__\":\n",
        "    converter = DocumentConverter()\n",
        "    doc = converter.convert(\"tm1_dg_dvlpr-10pages.pdf\").document\n",
        "    chunker = HybridChunker()\n",
        "    chunks = chunker.chunk(dl_doc=doc)\n",
        "\n",
        "    dataset = {}\n",
        "    for i, chunk in enumerate(chunks):\n",
        "            print(Fore.YELLOW + f\"Raw Text:\\n{chunk.text[:300]}…\" + Fore.RESET)\n",
        "            enriched_text = chunker.contextualize(chunk=chunk)\n",
        "            print(Fore.LIGHTMAGENTA_EX + f\"Contextualized Tex:\\n{enriched_text[:300]}…\" + Fore.RESET)\n",
        "\n",
        "            data = llm_call(\n",
        "                enriched_text\n",
        "            )\n",
        "            dataset[i] = {\"generated\":data[\"generated\"], \"context\":enriched_text}\n",
        "\n",
        "    with open('tm1data.json','w') as f:\n",
        "        json.dump(dataset, f)\n"
      ],
      "metadata": {
        "id": "ZJ73ceHc5T2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-4: After converting data into question and answer format === REFORMAT THEM USING CONTEXT FOR MORE ACCURATE FINETUNING"
      ],
      "metadata": {
        "id": "y402jp406M_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simply add chunk as context in the question so llm can also learn knowledge along with question\n",
        "#  = convert {question, answer} into {question = context:{chunk}+question, answer}\n",
        "\n",
        "import json\n",
        "from colorama import Fore\n",
        "\n",
        "instructions = []\n",
        "with open('tm1data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    for key,chunk in data.items():\n",
        "        for pairs in chunk['generated']:\n",
        "            question, answer = pairs['question'], pairs['answer']\n",
        "            context_pair = {\n",
        "                'question':f'{pairs['question']}',\n",
        "                'answer':pairs['answer']\n",
        "                }\n",
        "            instructions.append(context_pair)\n",
        "        print(Fore.YELLOW + str(chunk))\n",
        "        print('\\n~~~~~~~~~~~~~~~~~~~~~')\n",
        "\n",
        "with open('data/instruction.json','w') as f:\n",
        "    json.dump(instructions,f)\n",
        "\n",
        "with open('data/instruction.json','r') as f:\n",
        "    data = json.load(f)\n",
        "    print(Fore.LIGHTMAGENTA_EX + str(data[:10]))"
      ],
      "metadata": {
        "id": "H06qyAl567eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To run the fine-tuning file in a big gpu environment. Go to runpod.io -> fine a good gpu pod and click deploy -> copy ssh -> open local vs code and serach on top for ssh: connect to host and paste link give by deployed pod in run pod ( what ever code we execute in this folder of local vs code will get executed in runpod )**"
      ],
      "metadata": {
        "id": "Ek5Ih9tyDSEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-5: Create Chat-Template For our Question-answer Data ( to convert our data into llm's native training structure so it can understand our data easily )"
      ],
      "metadata": {
        "id": "w4ZNZhBy7UB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from colorama import Fore\n",
        "\n",
        "dataset = load_dataset(\"data\", split='train') # data folder has instruction.json where questiona dn answer are present\n",
        "print(Fore.YELLOW + str(dataset[2]) + Fore.RESET)  # print some of training data\n",
        "\n",
        "# CREATE A CHAT-TEMPLATE = Here the llm is trained on a strucute of tokens (system, user and assitant prompts) so we need ot resturcture our data into that format\n",
        "# if we don't resturcture and train llm on direct questiona dn answer, llm will hallucinate large since we destriiyed it training format and also destoryed the input in diiferent format so if doesn't even understand whats the input so output finetuing in such case increases hallucination\n",
        "\n",
        "def format_chat_template(batch, tokenizer):\n",
        "    system_prompt =  \"\"\"You are a helpful, honest and harmless assitant designed to help engineers. Think through each question logically and provide an answer. Don't make things up, if you're unable to answer a question advise the user that you're unable to answer as it is outside of your scope.\"\"\"\n",
        "    samples = []\n",
        "\n",
        "    # Seperate questions and answer from input\n",
        "    questions = batch[\"question\"]\n",
        "    answers = batch[\"answer\"]\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        # convert seperated question-answers into raw_json so we can use it to convert into chat-template\n",
        "        row_json = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": questions[i]},\n",
        "            {\"role\": \"assistant\", \"content\": answers[i]}\n",
        "        ]\n",
        "\n",
        "        # Apply chat template and append the result to the list\n",
        "        tokenizer.chat_template = \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n",
        "        text = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "        samples.append(text)\n",
        "\n",
        "    # Return a dictionary with lists as expected for batched processing\n",
        "    return {\n",
        "        \"instruction\": questions,\n",
        "        \"response\": answers,\n",
        "        \"text\": samples  # The processed chat template text for each row\n",
        "    }\n",
        "\n",
        "# By default the LLM Fine-tuning happens on samples (Chat-templates) not on seprated questions and asnwers"
      ],
      "metadata": {
        "id": "F6Sv2y_X84vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd4KQq5hHnI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-6: Fine-tune the model"
      ],
      "metadata": {
        "id": "wkDaUwcsFnso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import dependencies\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "import torch\n",
        "\n",
        "# usually we prefer a SLM to fine-tune than an llm since slm is fast, simple and far easily fine-tunable for something super niche\n",
        "base_model = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# tokenizer is not llm, it converts text into tokens for llm\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        base_model,\n",
        "        trust_remote_code=True, # to download model from HF Repo\n",
        "        token=\"HF_TOKEN_HERE\",\n",
        ")\n",
        "\n",
        "# Create Training Dataset from our question and answer dataset using above cell function\n",
        "# we are using tokenizer since tokenizer knows llm specific chat_template tokens structure\n",
        "train_dataset = dataset.map(lambda x: format_chat_template(x, tokenizer), num_proc=8, batched=True, batch_size=10) # num_proc defined how many processes to do parallely - good if very big dataset\n",
        "\n",
        "# quantization configuration\n",
        "# lets say model weight are 16bit flaot if we use 4 bit qunatization => if weight = 9.12345678902333341, we only finetune: 9.123 (crude and not too fine-tuneing but works)\n",
        "# 2-bit and 1-bit qunatizations are prone to hallucianating as they change a lot for everyupdate since then only update = 9.1 part of weight\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# get model from Hugginf face\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map=\"cuda:0\", # where model should run cpu or gpu = we can set it to \"auto\" it will also work if gpu is already present\n",
        "    quantization_config=quant_config, # quantization configuration\n",
        "    token=\"HF_TOKEN_HERE\",\n",
        "    cache_dir=\"./workspace\", # where exactly we load out model in the workspace\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# pass it through parameter efficent fine-tuning class to make to ready for fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LORA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    # big rank = more accurate fine-tunes and if rank is too low then accuracy is low and can introduce negligable finetuning\n",
        "    r=256, # rank determines the rank of matix we use for fine-tuning. EXAMPLE original weights are 8x8 and we use rank 2= instead of updating all 8x8 we update 8x2 and 2x8 matrices and then multiply them to get 8x8 weights\n",
        "    lora_alpha=512, # recommended to be a multple of rank and a higher value than rank\n",
        "    lora_dropout=0.05, # dropout to be better performer during inference\n",
        "    target_modules=\"all-linear\", # finetunes only lineer layer, if we do embedding layer then during inference it can be a bit of nightmare.\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# in peft config = increased rank and alpha increases accuracy\n",
        "\n",
        "# SFT = supervised fine-tuning\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    train_dataset=train_dataset,\n",
        "    args=SFTConfig(output_dir=\"meta-llama/Llama-3.2-1B-SFT\", num_train_epochs=50), # 50 is good but if larger or complex trainign data then increase epochs accordingly\n",
        "                              # fine-tuned model is stored in this output_dir\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "# fine-tuning the model using trainer\n",
        "trainer.train()\n",
        "\n",
        "# save the checkpoint and model\n",
        "trainer.save_model('complete_checkpoint')\n",
        "trainer.model.save_pretrained(\"final_model\")"
      ],
      "metadata": {
        "id": "2rthPIxmFn2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-7:  deploy locally using ilama = After training we have adapter-config.json and adaper_Safetensors = t=put them in a folder called checko]points and then use them inside olama to run the model and check fine-tuning**"
      ],
      "metadata": {
        "id": "bWFlmYE8NJhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KEY TAKEAWAYS:\n",
        "\n",
        "- increase rank and alpha inside peft will increases accuracy\n",
        "- Larger data is not always good, data quality is very important\n",
        "- Data should be more targetted than generic\n",
        "- Increasing epochs not always good, some may cause diminishing returns due to overfitting\n",
        "- More Quality data doen't need more Epoch runs"
      ],
      "metadata": {
        "id": "jiSSjLfjNyqi"
      }
    }
  ]
}